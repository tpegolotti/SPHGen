{# ./templates/library.template #}
#pragma once

#include <cstring>
#include <immintrin.h>

#define MCA_BEGIN __asm volatile("# LLVM-MCA-BEGIN eval":::"memory");
#define MCA_END __asm volatile("# LLVM-MCA-END":::"memory");

#define _PI {{ pi}}
#define _THETA {{ theta }}
#define _L {{ l}}
#define _LACC {{ l if not karatsuba else 2*l - 1}}
#define _LKEY {{ 2*l-1 if not karatsuba else l}}
#define _BIG {{ big }}
#define _SMALL {{ small }}
#define _KAPPA {{ kappa }}
#define _SIMD {{ simd_size }}
#define _BLOCK {{ block_size }}
#define _PIB {{ pib }}
#define SMALLR {{ smallR }}
#define _UNROLL {{ unroll }}
#define _CPI {{ cpi }}
#define _BIG_MASK {{ (2**big) - 1 }}
#define _SMALL_MASK {{ (2**small) - 1 }}
#define SMALLR_MASK {{ (2**smallR) - 1 }}

{% if simd_size == 4 -%}
typedef __m256i vtype;
{% else -%}
typedef __m512i vtype;
{% endif -%}

typedef union  bbigint
{
    vtype x[_L];
    unsigned long a[_SIMD * _L];
} bbigint;

typedef union  bkey
{
    vtype x[_LKEY];
    unsigned long a[_SIMD * ({{"_L + _L - 1" if not karatsuba else "_L"}})];
} bkey;


{% if ifma -%}

{% if karatsuba %}
typedef struct bacc
{
    vtype z0[{{2*(2*ceil(l/2)-1)}}]; // < _L lo, >=_L hi
    vtype z3[{{2*(2*ceil(l/2)-1)}}]; // < _L lo, >=_L hi
    vtype z2[{{2*(2*floor(l/2)-1)}}]; // < _L lo, >=_L hi
    vtype x[{{2*l}}];
    unsigned long a[_SIMD * (2*_LACC)];
} bacc;
{% else %}
typedef union  bacc
{
    vtype x[2*_LACC]; // < _L lo, >=_L hi
    unsigned long a[_SIMD * (2*_LACC)];
} bacc;
{% endif %}

typedef struct bigint { __int128 x[_L]; } bigint;
typedef __int128 ull;

std::ostream&
operator<<( std::ostream& dest, __int128_t value )
{
    std::ostream::sentry s( dest );
    if ( s ) {
        __uint128_t tmp = value < 0 ? -value : value;
        char buffer[ 128 ];
        char* d = std::end( buffer );
        do
        {
            -- d;
            *d = "0123456789"[ tmp % 10 ];
            tmp /= 10;
        } while ( tmp != 0 );
        if ( value < 0 ) {
            -- d;
            *d = '-';
        }
        int len = std::end( buffer ) - d;
        if ( dest.rdbuf()->sputn( d, len ) != len ) {
            dest.setstate( std::ios_base::badbit );
        }
    }
    return dest;
}

{% else -%}

{% if karatsuba %}

typedef struct
{
    vtype z0[{{2*ceil(l/2)-1}}];
    vtype z2[{{2*floor(l/2)-1}}];
    vtype z3[{{2*ceil(l/2)-1}}];
} bacc;

{% else %}

typedef union  bacc
{
    vtype x[_LACC];
    unsigned long a[_SIMD * (2*_L-1)];
} bacc;

{% endif %}

typedef struct bigint { unsigned long long x[_L]; } bigint;
typedef unsigned long long int ull;
{% endif -%}

void string_to_bigint(unsigned char* in, bigint* out){
    int mask_start = 255;
    int shift_start = 0;
    int cur = 0, remaining = _BIG, done = 0;
    for(int i = 0; i < _L; i++){
        out->x[i] = (in[cur] & mask_start) >> shift_start;
        cur++; remaining -= (8 - shift_start); done+=(8 - shift_start);
        while (remaining >= 8){
            out->x[i] |= in[cur] << done;
            done += 8; remaining -= 8; cur++;
        }
        if (remaining < 8){
            out->x[i] |= (in[cur] & ((1<<remaining)-1)) << done;
            mask_start = (255 - ((1<<remaining)-1));
            shift_start = remaining;
            done = 0; remaining = _BIG;
        }
    }
}

bool equal(bigint* a, bigint* b){
    for (int i = 0; i < _L; i++){
        if (a->x[i] != b->x[i]){
            return false;
        }
    }
    return true;
}

// TODO: I need to rewrite the scalar ops to use __int128 if I have ifma support
void mul(bigint* a, bigint* b, bigint* res){
    for (int i = 0; i < _L; i++){
        res->x[i] = 0;
    }
    for (int i = 0; i < _L; i++){
        for (int j = 0; j < i + 1; j++){
            res->x[i] += a->x[j] * b->x[i - j];
        }
        for (int j = i + 1; j < _L; j++){
            res->x[i] += a->x[j] * _KAPPA * b->x[_L - j + i];
        }
    }
}

void add(bigint* a, bigint* b){
    for (int i = 0; i < _L; i++){
        a->x[i] += b->x[i];
    }
}

void copy(bigint* a, bigint* b){
    for (int i = 0; i < _L; i++){
        a->x[i] = b->x[i];
    }
}

void fmadd(bigint* a, bigint* b, bigint* c){
    for (int i = 0; i < _L; i++){
        for (int j = 0; j < i + 1; j++){
            c->x[i] += a->x[j] * b->x[i - j];
        }
        for (int j = i + 1; j < _L; j++){
            c->x[i] += a->x[j] * _KAPPA * b->x[_L - j + i];
        }
    }
}

void carry_round(bigint* a){
    for (int i = 0; i < _L - 1; i++){
        a->x[i + 1] += a->x[i] >> _BIG;
        a->x[i] &= (1L << _BIG) - 1;
    }
    a->x[0] += (a->x[_L - 1] >> _SMALL) * _THETA;
    a->x[_L - 1] &= (1L << _SMALL) - 1;
    a->x[1] += a->x[0] >> _BIG;
    a->x[0] &= (1L << _BIG) - 1;
    a->x[2] += a->x[1] >> _BIG;
    a->x[1] &= (1L << _BIG) - 1;
}

void finalize(bigint *res, ull* FIN, bigint* KS){
    bigint temp[_SIMD-1];

    for(int i = 0; i < _SIMD-1; i++){
        for(int j = 0; j < _L; j++){
            temp[i].x[j] = FIN[j * _SIMD + i];
        }
    }
    for(int j = 0; j < _L; j++){
        res->x[j] = FIN[j * _SIMD + _SIMD - 1];
    }
    for(int i = 0; i < _SIMD-1; i++){
        fmadd(&temp[i], &KS[_SIMD-2-i], res);
        carry_round(res);
    }
}


{% if simd_size == 4 -%}

#define VMUL(a, b) (_mm256_mul_epu32(a, b))
#define VADD(a, b) (_mm256_add_epi64(a, b))
#define VSUB(a, b) (_mm256_sub_epi64(a, b))
#define VSRLI(a, imm) (_mm256_srli_epi64(a, imm))
#define VSLLI(a, imm) (_mm256_slli_epi64(a, imm))
#define VAND(a, b) (_mm256_and_si256(a, b))
#define VBROADCAST(a) (_mm256_set1_epi64x(a))
#define VOR(a, b) (_mm256_or_si256(a, b))
#define VSET(...) (_mm256_set_epi64x(__VA_ARGS__));
#define VGATHER(op0, op1, op2) _mm256_i64gather_epi64((long long int*)(op0), op1, op2)
#define VSTORE(op0, op1) _mm256_storeu_si256((vtype*)(op0), op1)
#define VSETZERO (_mm256_setzero_si256())
#define VINDEX VSET(3*_BLOCK, 2*_BLOCK, _BLOCK, 0)

{% else -%}

#define VMUL(a, b) (_mm512_mul_epu32(a, b))
#define VADD(a, b) (_mm512_add_epi64(a, b))
#define VSUB(a, b) (_mm512_sub_epi64(a, b))
#define VSRLI(a, imm) (_mm512_srli_epi64(a, imm))
#define VSLLI(a, imm) (_mm512_slli_epi64(a, imm))
#define VAND(a, b) (_mm512_and_si512(a, b))
#define VOR(a, b) (_mm512_or_si512(a, b))
#define VBROADCAST(a) (_mm512_set1_epi64(a))
#define VSET(...) (_mm512_set_epi64(__VA_ARGS__));
#define VGATHER(op0, op1, op2) _mm512_i64gather_epi64(op1, op0, op2)
#define VSTORE(op0, op1) _mm512_storeu_si512((vtype*)(op0), op1)
#define VSETZERO (_mm512_setzero_si512())
#define VINDEX VSET(7*_BLOCK, 6*_BLOCK, 5*_BLOCK, 4*_BLOCK, 3*_BLOCK, 2*_BLOCK, _BLOCK, 0)

{% endif -%}

{% if ifma-%}
#define VFMA(alo, ahi, b, c) \
    alo = _mm512_madd52lo_epu64(alo, b, c); \
    ahi = _mm512_madd52hi_epu64(ahi, b, c);
{%- else -%}
#define VFMA(a, b, c) \
    VADD(a, VMUL(b, c))
{%- endif %}

void load_scalar(bigint* out, unsigned char* in){
    #pragma unroll
    for (int i = 0; i < _L; i++)
        out->x[i] = *(ull*)(in + (i * _BIG >> 3));

    #pragma unroll
    for (int i = 0; i < _L; i++)
        out->x[i] >>= (i * _BIG) % 8;

    #pragma unroll
    for (int i = 0; i < _L - 1; i++)
        out->x[i] &= _BIG_MASK;

    out->x[_L-1] &= SMALLR_MASK;
    out->x[_L-1] |= (1ull << SMALLR);
}

#define ZERO(out) \
    {% for i in range(l) -%}
    out.x[{{i}}] = VSETZERO;\
    {% endfor %}

{% if karatsuba %}
{% if ifma %}
#define ZEROACC(out) \
    {% for i in range(2*(2*ceil(l/2)-1)) -%}
    out.z0[{{i}}] = VSETZERO;\
    {% endfor -%}
    {% for i in range(2*(2*ceil(l/2)-1)) -%}
    out.z3[{{i}}] = VSETZERO;\
    {% endfor -%}
    {% for i in range(2*(2*floor(l/2)-1)) -%}
    out.z2[{{i}}] = VSETZERO;\
    {% endfor -%}
    {% for i in range(2*l) -%}
    out.x[{{i}}] = VSETZERO;\
    {% endfor %}
{% else %}
#define ZEROACC(out) \
    {% for i in range(2*ceil(l/2)-1) -%}
    out.z0[{{i}}] = VSETZERO;\
    {% endfor -%}
    {% for i in range(2*ceil(l/2)-1) -%}
    out.z3[{{i}}] = VSETZERO;\
    {% endfor -%}
    {% for i in range(2*floor(l/2)-1) -%}
    out.z2[{{i}}] = VSETZERO;\
    {% endfor %}
{% endif %}
{% else %}
#define ZEROACC(out) \
    {% for i in range((l+((l-1)*karatsuba))*(2 if ifma else 1)) -%}
    out.x[{{i}}] = VSETZERO;\
    {% endfor %}
{% endif %}


// ignoring gather cycles which i think should be handled by the compiler + ooo execution, 4 cycles (probably can be ignored)
#define GATHER(out, start_addr) \
    {% for i in range(l) -%}
    out.x[{{i}}] = VGATHER(start_addr + ({{shift_right(i*big,3)}}), vindex, 1); \
    {% endfor -%}
    {% for i in range(l) -%}
    out.x[{{i}}] = VSRLI(out.x[{{i}}], ({{(i * big) % 8}})); \
    {% endfor -%}
    {% for i in range(l-1) -%}
    out.x[{{i}}] = VAND(out.x[{{i}}], bigv_mask); \
    {% endfor -%}
    out.x[{{l-1}}] = VAND(out.x[{{l-1}}], gather_small_mask); \
    out.x[{{l-1}}] = VOR(out.x[{{l-1}}], or_mask); \

{% if ifma %}

{% if karatsuba %}

#define MATMUL(C, A, K, first) \
    { \
    bkey KT; \
    {% for k in range(l) -%}
    {%- for i in range(ceil(l/2)) -%}
    {%- for j in range(ceil(l/2)) -%}
    {%- if i + j == k -%}
    VFMA(C.z0[{{k}}], C.z0[{{k+(2*ceil(l/2)-1)}}], A.x[{{i}}], K.x[{{j}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    {% for k in range(l) -%}
    {%- for i in range(floor(l/2)) -%}
    {%- for j in range(floor(l/2)) -%}
    {%- if i + j == k -%}
    VFMA(C.z2[{{k}}], C.z2[{{k+(2*floor(l/2)-1)}}], A.x[{{i+ceil(l/2)}}], K.x[{{j+ceil(l/2)}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    {%- for i in range(floor(l/2)) -%}
    A.x[{{i}}] = VADD(A.x[{{i}}], A.x[{{ceil(l/2) + i}}]); \
    {% endfor -%}
    {%- for i in range(floor(l/2)) -%}
    KT.x[{{i}}] = VADD(K.x[{{i}}], K.x[{{ceil(l/2) + i}}]); \
    {% endfor -%}
    {%- if l % 2 == 1 -%}
    KT.x[{{floor(l/2)}}] = K.x[{{floor(l/2)}}]; \
    {% endif -%}
    {# compute z3 -#}
    {% for k in range(l) -%}
    {%- for i in range(ceil(l/2)) -%}
    {%- for j in range(ceil(l/2)) -%}
    {%- if i + j == k -%}
    VFMA(C.z3[{{k}}], C.z3[{{k+(2*ceil(l/2)-1)}}], A.x[{{i}}], KT.x[{{j}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    }

{% else %}

// (C, A, K) => C += A * K
#define MATMUL(C, A, K, first) \
    {% for i in range(l) -%}
    {%- for j in range(l) -%}
    VFMA(C.x[{{i}}], C.x[{{i+l}}], A.x[{{j}}], K.x[{{l - 1 - i + j}}]); \
    {% endfor -%}
    {% endfor %}

{% endif %}
    
{% else %}

{% if karatsuba %}

#define MATMUL(C, A, K, first) \
    { \
    bkey KT; \
    {% for k in range(l) -%}
    {%- for i in range(ceil(l/2)) -%}
    {%- for j in range(ceil(l/2)) -%}
    {%- if i + j == k -%}
    C.z0[{{k}}] = VFMA(C.z0[{{k}}], A.x[{{i}}], K.x[{{j}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    {% for k in range(l) -%}
    {%- for i in range(floor(l/2)) -%}
    {%- for j in range(floor(l/2)) -%}
    {%- if i + j == k -%}
    C.z2[{{k}}] = VFMA(C.z2[{{k}}], A.x[{{i+ceil(l/2)}}], K.x[{{j+ceil(l/2)}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    {%- for i in range(floor(l/2)) -%}
    A.x[{{i}}] = VADD(A.x[{{i}}], A.x[{{ceil(l/2) + i}}]); \
    {% endfor -%}
    {%- for i in range(floor(l/2)) -%}
    KT.x[{{i}}] = VADD(K.x[{{i}}], K.x[{{ceil(l/2) + i}}]); \
    {% endfor -%}
    {%- if l % 2 == 1 -%}
    KT.x[{{floor(l/2)}}] = K.x[{{floor(l/2)}}]; \
    {% endif -%}
    {# compute z3 -#}
    {% for k in range(l) -%}
    {%- for i in range(ceil(l/2)) -%}
    {%- for j in range(ceil(l/2)) -%}
    {%- if i + j == k -%}
    C.z3[{{k}}] = VFMA(C.z3[{{k}}], A.x[{{i}}], KT.x[{{j}}]); \
    {% endif -%}
    {% endfor -%}
    {%- endfor -%}
    {% endfor -%}
    }

{% else %}
// (C, A, K) => C += A * K
#define MATMUL(C, A, K, first) \
    {% for i in range(l) -%}
    {%- for j in range(l) -%}
    {% if j == 0 -%}
    C.x[{{i}}] = first ? VMUL(A.x[{{j}}], K.x[{{l - 1 - i + j}}]) : VFMA(C.x[{{i}}], A.x[{{j}}], K.x[{{l - 1 - i + j}}]); \
    {%- else -%}
    C.x[{{i}}] = VFMA(C.x[{{i}}], A.x[{{j}}], K.x[{{l - 1 - i + j}}]); \
    {%- endif %}
    {% endfor -%}
    {% endfor %}
{% endif %}
{% endif %}

#define MULT_THETA(in, out) \
    {% set first = namespace(value=True) -%}
    {%- set prev = namespace(value=0) -%}
    {% for i, b in enumerate(reversed(bin(theta)[2:])) -%}
    {% if b == '1' -%}
    {%- if first.value -%}
    out = VSLLI(in, {{i}}); \
    {%- set prev.value = i -%}
    {%- set first.value = False -%}
    {%- else %}
    out = VADD(VSLLI(in, {{i}}), out); \
    {%- set prev.value = i -%}
    {%- endif -%}
    {%- endif -%}
    {% endfor %}

{% if karatsuba %}

// A is TMP, B is ACC
#define ELEMENTWISE_ADD(A, B) \
    {% for i in range(2*ceil(l/2)-1) -%}
    {% if i < 2*floor(l/2) - 1 -%}
    B.z3[{{i}}] = VSUB(B.z3[{{i}}], VADD(B.z0[{{i}}], B.z2[{{i}}])); \
    {% else -%}
    B.z3[{{i}}] = VSUB(B.z3[{{i}}], B.z0[{{i}}]); \
    {% endif -%}
    {% endfor -%}   
    {% for i in range(ceil(l/2)) -%}
    A.x[{{i}}] = VADD(B.z0[{{i}}], A.x[{{i}}]); \
    {% endfor -%}
    {% for i in range(ceil(l/2), 2*ceil(l/2)-1) -%}
    A.x[{{i}}] = VADD(VADD(B.z0[{{i}}], B.z3[{{i - ceil(l/2)}}]), A.x[{{i}}]); \
    {% endfor -%}

    {% for i in range(2*ceil(l/2)-1, l) -%}
    A.x[{{i}}] = VADD(B.z3[{{i - ceil(l/2)}}], A.x[{{i}}]); \
    {% endfor -%}

    vtype b_theta; \
    {% if l%2 -%}
    MULT_THETA(B.z3[{{floor(l/2)}}], b_theta) \
    {% if big != small -%}
    b_theta = VSLLI(b_theta, {{big - small}}); \
    {% endif -%}
    A.x[0] = VADD(b_theta, A.x[0]); \
    {% endif -%}


    {% for i in range(2*floor(l/2)-1) -%}
    {% if i < ceil(l/2)-1 -%}
    MULT_THETA(VADD(B.z2[{{i}}], B.z3[{{i+ceil(l/2)}}]), b_theta) \
    {% else -%}
    MULT_THETA(B.z2[{{i}}], b_theta) \
    {% endif -%}
    {% if big != small -%}
    b_theta = VSLLI(b_theta, {{big - small}}); \
    {% endif -%}
    A.x[{{i+l%2}}] = VADD(b_theta, A.x[{{i+l%2}}]); \
    {% endfor %}

{% else %}
// (A, B) => A += B
#define ELEMENTWISE_ADD(A, B) \
    {% for i in range(l) -%}
    A.x[{{i}}] = VADD(B.x[{{i}}], A.x[{{i}}]); \
    {% endfor %}
{% endif %}

{% if ifma %}

#define GET_CARRY(lo, hi, SHIFT) \
    VADD(VSRLI(lo, SHIFT), VSLLI(hi, 52 - SHIFT))

#define CARRYOP_FIRST(out, in) \
    out.x[1] = VADD(in.x[1], GET_CARRY(in.x[0], in.x[_L], _BIG)); \
    out.x[0] = VAND(in.x[0], bigv_mask);

#define CARRYOP(out, in, i) \
    out.x[i+1] = VADD(in.x[i+1], GET_CARRY(out.x[i], in.x[i+_L], _BIG)); \
    out.x[i] = VAND(out.x[i], bigv_mask);

#define CARRYOP_LAST(in, i) \
    in.x[i+1] = VADD(in.x[i+1], VSRLI(in.x[i], _BIG)); \
    in.x[i] = VAND(in.x[i], bigv_mask);

#define CARRY_ROUND(out, in) \
    CARRYOP_FIRST(out, in) \
    {% for i in range(1, l-1) -%}
    CARRYOP(out, in, {{i}}); \
    {% endfor -%}
    vtype last_shift = GET_CARRY(out.x[{{l-1}}], in.x[{{l-1+l}}], _SMALL); \
    vtype rlast; \
    MULT_THETA(last_shift, rlast); \
    out.x[{{l-1}}] = VAND(out.x[{{l-1}}], smallv_mask); \
    out.x[0] = VADD(out.x[0], rlast); \
    CARRYOP_LAST(out, 0); \
    CARRYOP_LAST(out, 1); \

{% else %}

#define GET_CARRY(in, SHIFT) \
    VSRLI(in, SHIFT)

// (in)
#define CARRYOP(in, i) \
    in.x[i+1] = VADD(in.x[i+1], GET_CARRY(in.x[i], _BIG)); \
    in.x[i] = VAND(in.x[i], bigv_mask);

#define CARRY_ROUND(in) \
    {% for i in range(l-1) -%}
    CARRYOP(in, {{i}}); \
    {% endfor -%}
    vtype last_shift = GET_CARRY(in.x[{{l-1}}], _SMALL); \
    vtype rlast; \
    MULT_THETA(last_shift, rlast); \
    in.x[{{l-1}}] = VAND(in.x[{{l-1}}], smallv_mask); \
    in.x[0] = VADD(in.x[0], rlast); \
    CARRYOP(in, 0); \
    CARRYOP(in, 1); 

{% endif %}

void eval(unsigned char* in, bigint key, bigint* res, int degree){
    int cur = 0;
    const vtype thetav = VBROADCAST(_THETA);
    const vtype bigv_mask = VBROADCAST((1L << _BIG) - 1);
    const vtype smallv_mask = VBROADCAST((1L << _SMALL) - 1);
    const vtype gather_small_mask = VBROADCAST((1L << SMALLR) - 1);
    const vtype or_mask = VBROADCAST((1ull << SMALLR));
    const vtype vindex = VINDEX;
    bbigint TMP;
    bacc ACC;
    {% if not karatsuba %}
    for (int i = 0; i < _L; i++){ACC.x[i] = VSETZERO;}
    {% endif %}

    bkey KM[_UNROLL];
    bigint Ks[_SIMD+_UNROLL];
    copy(&Ks[0], &key);
    for (int i = 1; i < _SIMD; i++){
        mul(&Ks[i-1], &key, &Ks[i]);
        carry_round(&Ks[i]);
    }
    for (int i = _SIMD; i < _SIMD+_UNROLL-1; i++){
        mul(&Ks[i-1], &Ks[_SIMD-1], &Ks[i]);
        carry_round(&Ks[i]);
    }
    {% if karatsuba %}
    for(int u = 0; u < _UNROLL; u++){
        for(int i = 0; i < _L; i++){
            KM[u].x[i] = VBROADCAST((unsigned long long int)Ks[_SIMD-1+u].x[i]);
        }
    }
    {% else %}
    for(int u = 0; u < _UNROLL; u++){
        for(int i = 0; i < _L; i++){
            KM[u].x[_L - 1 - i] = VBROADCAST((unsigned long long int)Ks[_SIMD-1+u].x[i]);
        }
        for(int i = 1; i < _L; i++){
            KM[u].x[_L + i - 1] = VBROADCAST((unsigned long long int)((ull)_KAPPA * Ks[_SIMD-1+u].x[_L - i]));
        }
    }
    {% endif %}

    // start computation
    if(degree >= _SIMD){
        GATHER(TMP, in);
        cur = _SIMD;
        for (; cur < degree - (_SIMD*_UNROLL) + 1; cur+=_SIMD*_UNROLL){
            {% if ifma or karatsuba %}
            ZEROACC(ACC)
            {% endif %}
            MATMUL(ACC, TMP, KM[_UNROLL-1], true);
            GATHER(TMP, in + cur * _BLOCK);
            {% for i in range(unroll-1) -%}
            MATMUL(ACC, TMP, KM[_UNROLL-{{2+i}}], false); 
            GATHER(TMP, in + (cur + {{i+1}}*_SIMD) * _BLOCK);
            {% endfor -%}
            {% if ifma -%}
            ELEMENTWISE_ADD(ACC, TMP);
            CARRY_ROUND(TMP, ACC);
            {% else %}
            ELEMENTWISE_ADD(TMP, ACC);
            CARRY_ROUND(TMP);
            {% endif %}
        }
        for (; cur < degree - _SIMD + 1; cur+=_SIMD){
            {% if ifma or karatsuba %}
            ZEROACC(ACC)
            {% endif %}
            MATMUL(ACC, TMP, KM[0], true);
            GATHER(TMP, in + cur * _BLOCK);
            {% if ifma -%}
            ELEMENTWISE_ADD(ACC, TMP);
            CARRY_ROUND(TMP, ACC);
            {% else %}
            ELEMENTWISE_ADD(TMP, ACC);
            CARRY_ROUND(TMP);
            {% endif %}
        }
        {% if ifma -%}
        ull FIN[_L * _SIMD]; 
        unsigned long long int TFIN[_L * _SIMD]; 
        {% for i in range(l) -%}
        VSTORE(&TFIN[{{i}}*_SIMD], TMP.x[{{i}}]); 
        {% endfor -%}
        for(int i = 0; i < _L * _SIMD; i++){ 
            FIN[i] = TFIN[i]; 
        }
        finalize(res, FIN, Ks);
        {% else -%}
        ull FIN[_L * _SIMD]; 
        {% for i in range(l) -%}
        VSTORE(&FIN[{{i}}*_SIMD], TMP.x[{{i}}]); 
        {% endfor -%}
        finalize(res, FIN, Ks);
        {%- endif %}
    }
    if(cur == 0){
        load_scalar(res, in);
        cur = 1;
    }
    bigint tmp;
    for (; cur < degree; cur+=1){
        load_scalar(&tmp, in + cur * _BLOCK);
        fmadd(res, &Ks[0], &tmp);
        carry_round(&tmp);
        copy(res, &tmp);
    }
    carry_round(res);
}
